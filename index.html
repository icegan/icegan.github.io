<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>ICE-GAN</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="icon" type="image/png" href="./index_files/images/icon.png">
    <link rel="stylesheet" href="./index_files/bootstrap.min.css">
    <link rel="stylesheet" href="./index_files/font-awesome.min.css">
    <link rel="stylesheet" href="./index_files/codemirror.min.css">
    <link rel="stylesheet" href="./index_files/app.css">
    <link rel="stylesheet" href="./index_files/bootstrap.min(1).css">

    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
	    tex2jax: {
	        inlineMath: [['$','$'], ['\\(','\\)']],
	        processEscapes: true
	    }
	});
    </script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LS3QXR96SJ"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-LS3QXR96SJ');
    </script>

    <script src="./index_files/jquery.min.js"></script>
    <script src="./index_files/bootstrap.min.js"></script>
    <script src="./index_files/codemirror.min.js"></script>
    <script src="./index_files/clipboard.min.js"></script>

    <script src="./index_files/app.js"></script>
</head>

<body data-gr-c-s-loaded="true">
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                ICE-GAN: Identity-aware and Capsule-Enhanced GAN with <br>Graph-based Reasoning for Micro-Expression Recognition and Synthesis
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        Jianhui Yu<sup>1</sup>
                    </li>

                    <li>
                        <a href="https://chaoyivision.github.io/" target='_blank'>
                          Chaoyi Zhang
                        </a><sup>1</sup>
                    </li>

                    <li>
                        <a href="http://www.cse.unsw.edu.au/~ysong/" target='_blank'>
                          Yang Song
                        </a><sup>2</sup>
                    </li>
                    <li>
                        <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/tom-cai.html" target='_blank'>
                          Weidong Cai
                        </a><sup>1</sup>
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <sup>1</sup>University of Sydney
                    </li>
                    <li>
                        <sup>2</sup>University of New South Wales
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/pdf/2005.04370.pdf" target='_blank'>
                            <img src="./index_files/images/cover_page.png" height="80px"><br>
                                <h4><strong>Main Paper</strong></h4>
                            </a>
                        </li>
<!--                        <li>-->
<!--                            <a href="./supplementary.pdf" target='_blank'>-->
<!--                            <img src="./index_files/images/supp.png" height="80px"><br>-->
<!--                                <h4><strong>Supp. Materials</strong></h4>-->
<!--                            </a>-->
<!--                        </li>-->
                        <li>
                            <a href="#video">
                            <img src="index_files/images/youtube_icon_dark.png" height="80px"><br>
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
<!--                         <li>-->
<!--                            <a href="#dataset">-->
<!--                            <img src="./index_files/images/data.png" height="80px"><br>-->
<!--                                <h4><strong>Data</strong></h4>-->
<!--                                <p style="color:blue;font-size:11px;">(available)</p>-->
<!--                                <p style="color:blue;font-size:11px;">[<a href="https://chaoyivision.github.io/SGGpoint/" target='_blank'>Dataset Description</a>]</p><br>-->
<!--                            </a>-->
<!--                        </li>-->
                        <li>
                            <a href="https://github.com/crane-papercode/ICE-GAN" target='_blank'>
                            <img src="./index_files/images/github_pad.png" height="80px"><br>
                                <h4><strong>Code</strong></h4> <p style="color:blue;font-size:11px;"></p>
                            </a>
                        </li>
                        <li>
                            <a href="#BibTeX">
                            <img src="./index_files/images/bibtex.jpg" height="80px"><br>
                                <h4><strong>BibTeX</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview
                </h3>
                <img src="./index_files/images/model_architecture.jpg" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    Micro-expressions are reflections of peopleâ€™s true feelings and motives, which attract an increasing
                    number of researchers into the study of automatic facial micro-expression recognition.
                    The short detection window, the subtle facial muscle movements, and the limited training samples
                    make micro-expression recognition challenging.
                    To this end, we propose a novel Identity-aware and Capsule-Enhanced Generative Adversarial Network
                    with graph-based reasoning (ICE-GAN), introducing micro-expression synthesis as an auxiliary task to
                    assist recognition. The generator produces synthetic faces with controllable micro-expressions and
                    identity-aware features, whose long-ranged dependencies are captured through the graph reasoning
                    module (GRM), and the discriminator detects the image authenticity and expression classes.
                    Our ICE-GAN was evaluated on Micro-Expression Grand Challenge 2019 (<b>MEGC2019</b>) with a significant
                    improvement (<b>12.9%</b>) over the winner and surpassed other state-of-the-art methods.
                </p>
            </div>
        </div>


        <div class="row" id="video">
            <div class="col-md-8 col-md-offset-2">
                <h3>Video</h3>
<!--                <p style="color:blue;font-size:11px;">(contains audio w/ subtitles)</p>-->
                <div class="text-center">
                    <p style="color:blue;font-size:14px;">Coming Soon</p>
<!--                    <video id="video_id" width="100%" controls="" controlsList="nodownload">>-->
<!--                        <source src="./video-SGGpoint.mp4" type="video/mp4">-->
<!--                        <track label="English" kind="subtitles" srclang="en" -->
<!--                               src="./@InProceedings{SGGpoint,-->
<!--                                                            author    = {Zhang, Chaoyi and Yu, Jianhui and Song, Yang and Cai, Weidong},-->
<!--                                                            title     = {Exploiting Edge-Oriented Reasoning for 3D Point-Based Scene Graph Analysis},-->
<!--                                                            booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},-->
<!--                                                            month     = {June},-->
<!--                                                            year      = {2021},-->
<!--                                                            pages     = {9705-9715}-->
<!--                                                        }SGGpoint.vtt">-->
<!--                    </video>-->

<!--                    <script type="text/javascript">-->
<!--                        $(document).ready(function() {-->
<!--                        var video = document.querySelector('#video_id'); // get the video element-->
<!--                        var tracks = video.textTracks; // one for each track element-->
<!--                        var track = tracks[0]; // corresponds to the first track element-->
<!--                        track.mode = 'hidden';});-->
<!--                    </script>-->

                </div>
            </div>
        </div>




        <div class="row" id="dataset">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Evaluation Benchmark
                </h3>
                <img src="./index_files/images/benchmark.png" class="img-responsive" alt="benchmark"><br>
<!--                <p class="text-justify">-->
<!--                    <ul>-->
<!--                        <li>-->
<!--                            <b>3D $\mathbf{SGG_{point}}$ on <span style="color:rgb(245, 125, 125)">Real-World</span> 3D Scans: 3DSSG-<font color="red">O27</font><font color="blue">R16</font></b> (based on 3RScan + 3DSSG).-->
<!--                            <ul>-->
<!--                                <li> 3D Scans (3RScan Dataset): <a href="https://arxiv.org/pdf/1908.06109.pdf" target='_blank'><i>paper</i></a>, <a href="https://waldjohannau.github.io/RIO/" target='_blank'><i>dataset</i></a> </li>-->
<!--                                <li> Scene Graph Annotations (3DSSG): <a href="https://arxiv.org/pdf/2004.03967.pdf" target='_blank'><i>paper</i></a>, <a href="https://3dssg.github.io/" target='_blank'><i>dataset</i></a> </li>-->
<!--                                <li> Our own preprocessed <b>3DSSG-<font color="red">O27</font><font color="blue">R16</font></b> dataset is available at <a href="https://cloudstor.aarnet.edu.au/plus/s/gmZxA7xNWGxjVDY" target='_blank'><i>HERE</i></a>. Note: our 3DSSG-<font color="red">O27</font><font color="blue">R16</font> is derived from 3RScan + 3DSSG, which was initially collected and annoated by <a href="http://campar.in.tum.de/Main/JohannaWald" target='_blank'>Johanna Wald Et Al</a>. To fully honour their data ownership and efforts in creating the datasets, please restrictly follow the 4-step instructions below to obtain it.-->
<!--                                    <ol>-->
<!--                                        <li> Fill in and submit this <a href="https://docs.google.com/forms/d/e/1FAIpQLSfl9Xm1qWiGmN2HXzbRIecVns_V-n-4bwrzPEE4ZezEpOKT9Q/viewform" target='_blank'>online form</a> (3RScan Terms of Use - as requested in <a href="https://waldjohannau.github.io/RIO/" target='_blank'>3RScan</a>) to register your interests in the datasets.-->
<!--                                        <li> A welcome email will be sent including full access to the raw 3RScan and 3DSSG datasets.-->
<!--                                        <li> Forward this welcome email to our <a href="mailto:SGGpoint@gmail.com">SGGpoint@gmail.com</a> to receive the <u>access password</u>.-->
<!--                                        <li> Go to the <a href="https://cloudstor.aarnet.edu.au/plus/s/gmZxA7xNWGxjVDY" target='_blank'>Download Page</a> to obtain our <b>3DSSG-<font color="red">O27</font><font color="blue">R16</font></b> dataset, where the detailed dataset info/structure could be found in <a href="https://chaoyivision.github.io/SGGpoint/" target='_blank'>Dataset Description</a>.-->
<!--                                    </ol>-->
<!--                            </ul>-->
<!--                        </li>-->
<!--                        <li>-->
<!--                            <b>3D $\mathbf{SGG_{point}}$ on <span style="color:rgb(143, 171, 219)">Synthetic</span> 3D Scenes: SUNCG + SceneGraphNet</b>.-->
<!--                            <ul>-->
<!--                                <li> 3D Scenes (SUNCG Dataset): <a href="https://arxiv.org/pdf/1611.08974.pdf" target='_blank'><i>paper</i></a></li>-->
<!--                                <li> Scene Graph Annotations (SceneGraphNet): <a href="https://arxiv.org/pdf/1907.11308.pdf" target='_blank'><i>paper</i></a>, <a href="https://github.com/yzhou359/3DIndoor-SceneGraphNet/#Dataset" target='_blank'><i>dataset</i></a>, <a href="https://people.umass.edu/~yangzhou/scenegraphnet/" target='_blank'><i>project</i></a></li>-->
<!--                            </ul>-->
<!--                        </li>-->
<!--                        <li>-->
<!--                            <b>Traditional Graph Representation Learning: Planetoid + MoleculeNet.</b>-->
<!--                            <ul>-->
<!--                                <li> Node classification for citation analysis on Planetoid dataset (Cora, CiteSeer, and Pubmed): <a href="https://arxiv.org/pdf/1603.08861.pdf" target='_blank'><i>paper</i></a>, <a href="https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#learning-methods-on-graphs" target='_blank'><i>script</i></a>-->
<!--                                <li> Graph recognition for molecular analysis on MoleculeNet dataset (Tox21 and BBBP): <a href="https://arxiv.org/pdf/1703.00564.pdf" target='_blank'><i>paper</i></a>, <a href="https://github.com/awslabs/dgl-lifesci/blob/master/examples/property_prediction/moleculenet/classification.py" target='_blank'><i>script</i></a>-->
<!--                            </ul>-->
<!--                        </li>-->
<!--                    </ul>-->
<!--                </p>-->
            </div>
        </div>

        <div class="row" id="BibTeX">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
               If you find our project useful in your research, please cite:
                <pre class="w3-panel w3-leftbar w3-light-grey" style="white-space: pre-wrap; font-family: monospace; font-size: 10px">
@InProceedings{yu2020ice,
    title     = {ICE-GAN: Identity-aware and Capsule-Enhanced GAN with Graph-based Reasoning for Micro-Expression Recognition and
                 Synthesis},
    author    = {Yu, Jianhui and Zhang, Chaoyi and Song, Yang and Cai, Weidong},
    booktitle = {2021 International Joint Conference on Neural Networks, {IJCNN} 2021}
    year      = {2021}
}</pre>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgments
                </h3>
                The website template was borrowed from <a href="https://ucsd-openrooms.github.io/" target='_blank'>Zhenqin Li's project</a>.
                <p></p>
            </div>
        </div>
    </div>


</body></html>
