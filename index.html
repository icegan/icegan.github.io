<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>SGGpoint</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="icon" type="image/png" href="./index_files/images/icon.png">
    <link rel="stylesheet" href="./index_files/bootstrap.min.css">
    <link rel="stylesheet" href="./index_files/font-awesome.min.css">
    <link rel="stylesheet" href="./index_files/codemirror.min.css">
    <link rel="stylesheet" href="./index_files/app.css">
    <link rel="stylesheet" href="./index_files/bootstrap.min(1).css">

    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
	    tex2jax: {
	        inlineMath: [['$','$'], ['\\(','\\)']],
	        processEscapes: true
	    }
	});
    </script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LS3QXR96SJ"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-LS3QXR96SJ');
    </script>

    <script src="./index_files/jquery.min.js"></script>
    <script src="./index_files/bootstrap.min.js"></script>
    <script src="./index_files/codemirror.min.js"></script>
    <script src="./index_files/clipboard.min.js"></script>

    <script src="./index_files/app.js"></script>
</head>

<body data-gr-c-s-loaded="true">
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                Exploiting Edge-Oriented Reasoning for 3D Point-based<br>Scene Graph Analysis
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://chaoyivision.github.io/" target='_blank'>
                          Chaoyi Zhang
                        </a><sup>1</sup>
                    </li>
                    <li>
                        Jianhui Yu<sup>1</sup>
                    </li>
                    <li>
                        <a href="http://www.cse.unsw.edu.au/~ysong/" target='_blank'>
                          Yang Song
                        </a><sup>2</sup>
                    </li>
                    <li>
                        <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/tom-cai.html" target='_blank'>
                          Weidong Cai
                        </a><sup>1</sup>
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <sup>1</sup>University of Sydney
                    </li>
                    <li>
                        <sup>2</sup>University of New South Wales
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/pdf/2103.05558.pdf" target='_blank'>
                            <img src="./index_files/images/paper.png" height="80px"><br>
                                <h4><strong>Main Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="./supplementary.pdf" target='_blank'>
                            <img src="./index_files/images/supp.png" height="80px"><br>
                                <h4><strong>Supp. Materials</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="#video">
                            <img src="index_files/images/youtube_icon_dark.png" height="80px"><br>
                                <h4><strong>Video</strong></h4> <p style="color:blue;font-size:11px;">(Coming soon)</p>
                            </a>
                        </li>
                         <li>
                            <a href="#dataset">
                            <img src="./index_files/images/data.png" height="80px"><br>
                                <h4><strong>Data</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <img src="./index_files/images/github_pad.png" height="80px"><br>
                                <h4><strong>Code</strong></h4> <p style="color:blue;font-size:11px;">(Coming soon)</p>
                            </a>
                        </li>
                        <li>
                            <a href="#BibTeX">
                            <img src="./index_files/images/bibtex.jpg" height="80px"><br>
                                <h4><strong>BibTeX</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview
                </h3>
                <img src="./index_files/images/teaser.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                Scene understanding is a critical problem in computer vision.
In this paper, we propose a 3D point-based scene graph generation (\(\bf{SGG_{point}}\)) framework to effectively bridge perception and reasoning to achieve scene understanding via three sequential stages, namely scene graph construction, reasoning, and inference.
Within the reasoning stage, an EDGE-oriented Graph Convolutional Network (\(\texttt{EdgeGCN}\)) is created to exploit multi-dimensional edge features for explicit relationship modeling, together with the exploration of two associated twinning interaction mechanisms between nodes and edges for the independent evolution of scene graph representations.
Overall, our integrated \(\bf{SGG_{point}}\) framework is established to seek and infer scene structures of interest from both real-world and synthetic 3D point-based scenes.
Our experimental results show promising edge-oriented reasoning effects on scene graph generation studies. We also demonstrate our method advantage on several traditional graph representation learning benchmark datasets, including the node-wise classification on citation networks and whole-graph recognition problems for molecular analysis.</p>
            </div>
        </div>


        <div class="row" id="video">
            <div class="col-md-8 col-md-offset-2">
                <h3>Video</h3><p style="color:blue;font-size:11px;">(Coming soon)</p>
                <div class="text-center">
                    <video id="v0" width="100%" autoplay="" loop="" muted="" controls="">
                        <source src="" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>

        <br><br>


        <div class="row" id="dataset">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Datasets & Evaluation Scripts
                </h3>
                <p class="text-justify">
                    <ul>
                        <li>
                            <b>3D $\mathbf{SGG_{point}}$ on <span style="color:rgb(245, 125, 125)">Real-World</span> 3D Scans: 3DSSG-O27R16</b> (based on 3RScan + 3DSSG).
                            <ul>
                                <li> 3D Scans (3RScan Dataset): <a href="https://arxiv.org/pdf/1908.06109.pdf" target='_blank'><i>paper</i></a>, <a href="https://waldjohannau.github.io/RIO/" target='_blank'><i>dataset</i></a> </li>
                                <li> Scene Graph Annotations (3DSSG): <a href="https://arxiv.org/pdf/2004.03967.pdf" target='_blank'><i>paper</i></a>, <a href="https://3dssg.github.io/" target='_blank'><i>dataset</i></a> </li>
                                <li> Our own preprocessed <b>3DSSG-O27R16</b> dataset (<a href="https://chaoyivision.github.io/SGGpoint/" target='_blank'>dataset description</a>) is available at <a href="https://cloudstor.aarnet.edu.au/plus/s/gmZxA7xNWGxjVDY" target='_blank'><i>here</i></a>. Note: our 3DSSG-O27R16 is derived from 3RScan + 3DSSG, which was initially collected and annoated by <a href="https://chaoyivision.github.io/SGGpoint/" target='_blank'>Johanna Wald</a>. To fully honour their data ownership and efforts in creating the datasets, please restrictly follow the 3-step instructions below to receive the <u>access password</u>.
                                    <ol>
                                        <li> Fill in and sumit this online <a href="https://docs.google.com/forms/d/e/1FAIpQLSfl9Xm1qWiGmN2HXzbRIecVns_V-n-4bwrzPEE4ZezEpOKT9Q/viewform" target='_blank'>form</a> (3RScan Terms of Use - as requested in <a href="https://docs.google.com/forms/d/e/1FAIpQLSfl9Xm1qWiGmN2HXzbRIecVns_V-n-4bwrzPEE4ZezEpOKT9Q/viewform" target='_blank'>3RScan</a>) to register your interests in the datasets.
                                        <li> A welcome email will be received including full access to obtain the raw 3RScan and 3DSSG dataset.
                                        <li> Forward this welcome email to <a href="mailto:SGGpoint@gmail.com">SGGpoint@gmail.com</a> to receive the receive the access password.
                                    </ol>
                            </ul>
                        </li>
                        <li>
                            <b>3D $\mathbf{SGG_{point}}$ on <span style="color:rgb(143, 171, 219)">Synthetic</span> 3D Scenes: SUNCG + SceneGraphNet</b>.
                            <ul>
                                <li> 3D Scenes (SUNCG Dataset): <a href="https://arxiv.org/pdf/1611.08974.pdf" target='_blank'><i>paper</i></a></li>
                                <li> Scene Graph Annotations (SceneGraphNet): <a href="https://arxiv.org/pdf/1907.11308.pdf" target='_blank'><i>paper</i></a>, <a href="https://github.com/yzhou359/3DIndoor-SceneGraphNet/#Dataset" target='_blank'><i>dataset</i></a>, <a href="https://people.umass.edu/~yangzhou/scenegraphnet/" target='_blank'><i>project</i></a></li>
                            </ul>
                        </li>
                        <li>
                            <b>Traditional Graph Representation Learning: Planetoid + MoleculeNet.</b>
                            <ul>
                                <li> Node classification for citation analysis on Planetoid dataset (Cora, CiteSeer, and Pubmed): <a href="https://arxiv.org/pdf/1603.08861.pdf" target='_blank'><i>paper</i></a>, <a href="https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#learning-methods-on-graphs" target='_blank'><i>script</i></a>
                                <li> Graph recognition for molecular analysis on MoleculeNet dataset (Tox21 and BBBP): <a href="https://arxiv.org/pdf/1703.00564.pdf" target='_blank'><i>paper</i></a>, <a href="https://github.com/awslabs/dgl-lifesci/blob/master/examples/property_prediction/moleculenet/classification.py" target='_blank'><i>script</i></a>
                            </ul>
                        </li>
                    </ul>
                </p>
            </div>
        </div>

         <div class="row" id="BibTeX">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
               If you find our data or project useful in your research, please cite:
                <pre class="w3-panel w3-leftbar w3-light-grey" style="white-space: pre-wrap; font-family: monospace; font-size: 10px">
@inproceedings{SGGpoint,
    title = {Exploiting Edge-Oriented Reasoning for 3D Point-based Scene Graph Analysis},
    author = {Zhang, Chaoyi and Yu, Jianhui and Song, Yang and Cai, Weidong},
    booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2021}
}</pre>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgments
                </h3>
                The website template was borrowed from <a href="https://ucsd-openrooms.github.io/" target='_blank'>Zhenqin Li's project</a>.
                <p></p>
            </div>
        </div>
    </div>


</body></html>
